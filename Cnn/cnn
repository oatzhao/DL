一般的卷积神经网络由多个卷积层构成，每个卷积层中通常会有一下几个操作：

（1）图像通过多个不同的卷积核的滤波，并加偏置，提取出局部特征，每一个卷积核会映射出一个新的2D图像

（2）将前面卷积和的滤波输出结果，进行非线性的激活函数处理。目前最常见的是使用ReLU函数，而以前Sigmoid函数较多

（3）对激活函数的结果再进行池化操作（即降采样，比如2 X 2的图片将为1 X 1的图片），目前一般是使用最大的池化，

目前，一般是使用最大池化，保留最显著的特征，并提升模型的变形容忍能力


全连接核心操作就是矩阵向量乘积：

y = Wx

本质就是由一个特征空间线性变变换到另一个特征空间，目标空间的任何一维也就是隐藏层的一个cell都会认为会受到原空间的每一维的影响

粗略的说，目标向量是原向量的加权和

卷积神经网络的要点：

（1）局部连接

（2）权值共享

（3）池化层



