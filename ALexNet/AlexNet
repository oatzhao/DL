LRN Local Response Normalization

局部响应归一化

AlexNet将LeNet的思想发扬光大，把CNN的基本原理应用到很深很宽的网络中。AlexNet具有一下新技术：

（1）成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmod，成功解决了Sigmoid在网络较深

时的梯度弥散问题。

（2）训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。


（3）在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet中全部使用最大池化，避免平均池化的模糊化效果。

并且Alex中提出让步长比池化核的尺寸小，这样池化层之间会有重叠核覆盖，提升了特征的丰富性。

（4）提出LRN层，对局部神经元的活动创建竞争机制，是的其中响应比较大的值变得相对更大，并一直其他反馈较小的神经元，增强了模型的泛化能力

（5）使用CUDA加速深度卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算

（6）数据增强，随机地从256 * 256的原始图像中截取224 * 224大小的区域，相当增加了sqrt(（256-224）) * 2=2048倍的数据量

如果没有数据增强，仅靠原始数据的数据量，参数众多的CNN会陷入过拟合中，使用了数据增强后可以大大减少过拟合，提升泛化能力。

进行预测时候，则是取图片的四个角加中间共5个位置，并进行左右翻转，一共获得10张图片，对他们进行预测并对10次结果求均值。同时，

AlexNet论文中提到了会对图像的RGB数据进行PCA处理，并对主要成分做一个标准为0.1的高斯扰动，增加一些噪音，这个Trick可以让错误率再下降1%

运行结果：

/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/bin/python2.7 /Users/zhaoyan20/PycharmProjects/DL/ALexNet/AlexNet.py
(u'conv1', ' ', [32, 56, 56, 64])
None
(u'pool1', ' ', [32, 27, 27, 64])
(u'conv2', ' ', [32, 27, 27, 192])
(u'pool2', ' ', [32, 13, 13, 192])
(u'conv3', ' ', [32, 13, 13, 384])
(u'conv4', ' ', [32, 13, 13, 256])
(u'conv5', ' ', [32, 13, 13, 256])
(u'pool5', ' ', [32, 6, 6, 256])
2018-04-01 12:35:05.761367: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-04-01 12:35:10.196195: step 0, duration = 0.392
2018-04-01 12:35:14.194951: step 10, duration = 0.403
2018-04-01 12:35:18.143499: step 20, duration = 0.382
2018-04-01 12:35:22.077110: step 30, duration = 0.423
2018-04-01 12:35:26.179599: step 40, duration = 0.407
2018-04-01 12:35:30.429627: step 50, duration = 0.411
2018-04-01 12:35:34.536754: step 60, duration = 0.417
2018-04-01 12:35:38.718879: step 70, duration = 0.419
2018-04-01 12:35:42.964130: step 80, duration = 0.424
2018-04-01 12:35:47.249452: step 90, duration = 0.420
2018-04-01 12:35:51.128735: Fowward across 100 steps, 0.453 + /- 0.049 sec / batch
2018-04-01 12:36:05.945306: step 0, duration = 1.433
2018-04-01 12:36:18.846485: step 10, duration = 1.245
2018-04-01 12:36:31.296434: step 20, duration = 1.222
2018-04-01 12:36:44.305762: step 30, duration = 1.256
2018-04-01 12:36:58.968062: step 40, duration = 1.332
2018-04-01 12:37:11.326236: step 50, duration = 1.230
2018-04-01 12:37:25.386875: step 60, duration = 1.515
2018-04-01 12:37:40.840789: step 70, duration = 1.305
2018-04-01 12:37:53.683089: step 80, duration = 1.243
2018-04-01 12:38:08.798142: step 90, duration = 1.380
2018-04-01 12:38:21.891553: Forward-backward across 100 steps, 1.507 + /- 0.208 sec / batch

Process finished with exit code 0

